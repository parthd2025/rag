# RAG Chatbot - Production-Ready Implementation

A complete, fully offline, free RAG (Retrieval-Augmented Generation) document-helper chatbot application.

## âœ¨ Features

- **100% Free & Offline** - No paid APIs, no internet required after setup
- **Multi-Format Support** - Upload PDFs, DOCX, TXT, and Markdown files
- **Fast Embeddings** - Uses Sentence Transformers for semantic search
- **Local LLM** - Run Mistral, Phi-3, or Qwen models locally
- **Web Interface** - Clean Streamlit UI for easy interaction
- **REST API** - FastAPI backend for programmatic access
- **Production-Ready** - Full error handling and CORS support
- **Persistent Storage** - FAISS vector database with on-disk persistence

## ğŸ—ï¸ Architecture

```
RAG Chatbot
â”œâ”€â”€ Backend (FastAPI)
â”‚   â”œâ”€â”€ Document Ingestion (PDFs, DOCX, TXT)
â”‚   â”œâ”€â”€ Vector Store (FAISS)
â”‚   â”œâ”€â”€ Embeddings (Sentence Transformers)
â”‚   â””â”€â”€ LLM Engine (Local models via llama-cpp-python)
â”‚
â”œâ”€â”€ Frontend (Streamlit)
â”‚   â”œâ”€â”€ File Upload
â”‚   â”œâ”€â”€ Chat Interface
â”‚   â”œâ”€â”€ Document Management
â”‚   â””â”€â”€ Statistics Dashboard
â”‚
â””â”€â”€ Storage
    â”œâ”€â”€ Vector Embeddings (data/embeddings/)
    â””â”€â”€ Document Cache (data/documents/)
```

## ğŸ“‹ System Requirements

- **Python 3.10+**
- **RAM**: 4GB minimum (8GB+ recommended for smooth LLM inference)
- **Disk Space**: 
  - FAISS index + embeddings: ~200MB per 100K tokens
  - Model (Q4_K_M): ~4-5GB
  - Total: ~10-15GB for full setup

## ğŸš€ Quick Start

### Step 1: Install Dependencies

```bash
# Navigate to project root
cd d:\RAG

# Install all requirements
pip install -r requirements.txt
```

### Step 2: Download a Local LLM Model

Download a quantized GGUF model. Recommended: **Mistral-7B-Instruct**

**Option A: Manual Download (Recommended)**
1. Visit: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF
2. Download `mistral-7b-instruct-v0.2.Q4_K_M.gguf` (~4.8GB)
3. Place in `models/` directory

**Option B: Automated Download (requires Git LFS)**
```bash
# Install git-lfs first: https://git-lfs.com
git lfs install
git clone https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF
mv Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf models/
```

### Step 3: Start the Backend

```bash
cd backend
python main.py
```

Expected output:
```
Initializing RAG components...
Loading model from models/mistral-7b-instruct-v0.2.Q4_K_M.gguf...
Model loaded successfully
INFO:     Uvicorn running on http://0.0.0.0:8000
```

The API is now running at `http://localhost:8000`

### Step 4: Start the Frontend

In a new terminal:

```bash
cd frontend
streamlit run app.py
```

This opens a browser at `http://localhost:8501`

## ğŸ“š Usage

### Via Streamlit UI (Recommended)

1. **Upload Documents**
   - Click "Upload Documents" in sidebar
   - Select PDF, DOCX, TXT, or Markdown files
   - Click "ğŸ“¤ Upload Documents"

2. **Ask Questions**
   - Type your question in the chat box
   - Click "Send" to get an answer
   - View sources in the expandable section

3. **Manage Documents**
   - View loaded documents in sidebar
   - Clear documents with "ğŸ—‘ï¸ Clear All Documents" button
   - Check statistics and settings

### Via REST API

**Health Check:**
```bash
curl http://localhost:8000/health
```

**Upload Documents:**
```bash
curl -X POST http://localhost:8000/upload \
  -F "files=@document.pdf" \
  -F "files=@document2.docx"
```

**Query:**
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "What is this about?", "top_k": 5}'
```

**List Documents:**
```bash
curl http://localhost:8000/documents
```

**Clear Documents:**
```bash
curl -X DELETE http://localhost:8000/clear \
  -H "Content-Type: application/json" \
  -d '{"confirm": true}'
```

**Get Statistics:**
```bash
curl http://localhost:8000/stats
```

## ğŸ“ Project Structure

```
RAG/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI server
â”‚   â”œâ”€â”€ ingest.py              # Document processing
â”‚   â”œâ”€â”€ vectorstore.py         # FAISS vector store
â”‚   â”œâ”€â”€ llm_loader.py          # LLM engine
â”‚   â”œâ”€â”€ rag_engine.py          # RAG orchestration
â”‚   â”œâ”€â”€ requirements.txt        # Backend dependencies
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                 # Streamlit UI
â”‚   â””â”€â”€ .streamlit/
â”‚       â””â”€â”€ config.toml        # Streamlit config (optional)
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ README.md              # Model download instructions
â”‚   â””â”€â”€ (place GGUF models here)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/             # Temporary upload cache
â”‚   â””â”€â”€ embeddings/            # FAISS index & metadata
â”‚
â”œâ”€â”€ requirements.txt           # Root requirements
â””â”€â”€ README.md                  # This file
```

## âš™ï¸ Configuration

### LLM Model Selection

Edit `backend/main.py` line ~50 to change the LLM:

```python
# For llama-cpp-python (local GGUF models)
llm_engine = LocalLLMEngine(
    model_path="models/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
    n_ctx=4096,
    n_threads=4,
    n_gpu_layers=0  # Set to >0 if using CUDA
)

# For HuggingFace Transformers (larger, slower)
# llm_engine = HuggingFacePipelineLLM(
#     model_name="mistralai/Mistral-7B-Instruct-v0.2",
#     device="cpu"  # or "cuda"
# )
```

### Embedding Model

Default: `all-MiniLM-L6-v2` (lightweight, fast)

To change, edit `backend/main.py`:
```python
vector_store = FAISSVectorStore(
    embedding_model_name="all-MiniLM-L6-v2"  # or other sentence-transformers models
)
```

### API Configuration

Edit `backend/main.py` for:
- **Host/Port**: Change `uvicorn.run()` parameters (line ~250)
- **CORS Origins**: Modify `CORSMiddleware` (line ~50)
- **Chunk Size**: Change `DocumentIngestor(chunk_size=1000)`

### Frontend Configuration

Create `.streamlit/config.toml`:
```toml
[server]
port = 8501
headless = true

[client]
showErrorDetails = true
```

## ğŸ”§ Troubleshooting

### Issue: "Model not loaded" error

**Solution:**
1. Verify model file exists in `models/` directory
2. Check model path in `backend/main.py` matches actual filename
3. Ensure model is GGUF format
4. Check disk space (Q4 models need ~5GB)

### Issue: High memory usage during inference

**Solution:**
1. Reduce model size (use Q4 quantization instead of Q8)
2. Reduce context window: `n_ctx=2048` instead of 4096
3. Use smaller models: Phi-3-Mini instead of Mistral-7B

### Issue: Slow embedding generation

**Solution:**
1. Use GPU: install CUDA and set `device="cuda"`
2. Use smaller embedding model: `"all-MiniLM-L6-v2"` (default is already optimized)
3. Increase `n_threads` in LLM config

### Issue: "Cannot connect to API" in Streamlit

**Solution:**
1. Ensure backend is running: `python backend/main.py`
2. Check API is accessible: `curl http://localhost:8000/health`
3. Update API URL in frontend: change `API_BASE_URL` in `frontend/app.py`

### Issue: FAISS index corrupted

**Solution:**
```bash
# Delete corrupted files
rm data/embeddings/faiss.index
rm data/embeddings/metadata.json

# Restart backend - new index will be created
python backend/main.py
```

## ğŸ“Š Performance Benchmarks

Typical performance on 4-core CPU, 8GB RAM:

| Task | Time | Notes |
|------|------|-------|
| Embedding 1000 chunks | 2-3s | Sentence Transformers |
| Vector search | 50-100ms | FAISS FlatL2 |
| LLM generation (100 tokens) | 10-30s | Mistral-7B Q4, CPU-only |
| Document upload & ingest | 1-5s | Depends on file size |

**Tips to improve performance:**
- Use GPU for LLM (10x faster): set `n_gpu_layers > 0`
- Use smaller context window: `n_ctx=2048`
- Reduce top_k for retrieval (default is fine)

## ğŸ”’ Security Notes

- **No authentication by default** - suitable for local/trusted network use
- **CORS enabled for all origins** - restrict in production
- **No input validation** - add if exposing to untrusted users
- **File upload limits** - set in FastAPI config

For production deployment:
1. Add authentication (JWT, OAuth)
2. Restrict CORS origins
3. Add rate limiting
4. Validate file uploads
5. Use HTTPS (reverse proxy like nginx)
6. Run behind a load balancer

## ğŸ“ License

Free to use locally. Original project structure and all code provided under MIT License.

## ğŸ™‹ FAQ

**Q: Can I use a cloud LLM API instead?**
A: Yes, but this defeats the purpose of "offline." The current implementation is designed for complete offline operation.

**Q: Can I use this in production?**
A: Yes, with security hardening (see Security Notes). It's suitable for internal company use, not public-facing without additional security.

**Q: How many documents can I load?**
A: Limited mainly by disk space. A 10GB embedding database can hold ~50M chunks.

**Q: Can I export the vector database?**
A: Yes, `data/embeddings/faiss.index` and `data/embeddings/metadata.json` can be backed up or shared.

**Q: Is GPU support required?**
A: No, it's optional. CPU inference works fine for small-to-medium documents.

## ğŸš€ Next Steps

1. [Download a model](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)
2. [Start the backend](#step-3-start-the-backend)
3. [Start the frontend](#step-4-start-the-frontend)
4. Upload your documents
5. Ask questions!

---

**Built with:** FastAPI â€¢ Streamlit â€¢ FAISS â€¢ Sentence Transformers â€¢ llama-cpp-python
