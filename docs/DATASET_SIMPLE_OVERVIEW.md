# ğŸ“Œ Dataset Overview - Simple Explanation

## Your Question Answered

> **"How to check the dataset and which entries are getting stored in and how? If I fire a query in streamlit then what will be the dataset role in the whole process?"**

---

## ğŸ¯ Quick Summary

### What is Dataset?
A **collection of test cases** (question-answer pairs with expected answers)

### Where is it Stored?
`data/datasets/` folder with JSON files

### What Gets Stored?
- Questions
- Expected/correct answers
- Context/reference info
- Difficulty level (easy/medium/hard)
- Category/topic
- Source documents

### Dataset's Role When You Fire Query:

```
Query in Streamlit
        â†“
    RAG Engine generates answer
        â†“
[OPTIONAL] Dataset evaluates: "Is the RAG answer correct?"
        â†“
Returns evaluation score (0-100%)
```

---

## ğŸ“Š How to Check Dataset

### Option 1: Via Command (Easiest)
```bash
# See all datasets
python scripts/opik/dataset_management.py list-datasets

# See details of one dataset
python scripts/opik/dataset_management.py get-dataset --dataset-id <id>
```

### Option 2: View Files Directly
```bash
# See storage location
ls data\datasets\

# View metadata
type data\datasets\dataset_xyz\metadata.json

# View test cases
python -m json.tool data\datasets\dataset_xyz\testcases.json
```

### Option 3: Use Python
```python
from src.backend.services.dataset_service import DatasetService

service = DatasetService()
datasets = service.get_all_datasets()
for ds in datasets:
    test_cases = service.get_test_cases(ds.id)
    print(f"{ds.name}: {len(test_cases)} test cases")
```

---

## ğŸ“ˆ Example: Real Data Stored

### Dataset 1: Automotive Q&A
```
Stored Location: data/datasets/dataset_automotive_qa_1704871200123/

Test Cases (Examples):
1. Q: "What is the maximum engine displacement?"
   A: "The maximum engine displacement is 5.0L"
   Difficulty: easy
   Category: engine

2. Q: "How does regenerative braking work?"
   A: "Regenerative braking recovers kinetic energy..."
   Difficulty: hard
   Category: safety

3. Q: "What is transmission fluid capacity?"
   A: "The transmission fluid capacity is 8.5 quarts"
   Difficulty: medium
   Category: transmission

Total: 50 test cases
```

---

## ğŸ”„ Query Flow with Dataset Role

### When User Fires Query in Streamlit:

**Step 1: User Enters Query**
```
Streamlit: "What is the maximum engine displacement?"
```

**Step 2: Backend Processes (RAG Engine)**
```
1. Search FAISS index for relevant documents
2. Send query + context to LLM (Groq)
3. Get answer: "The max is 5.0L"
```

**Step 3: Dataset Evaluates (If Provided)**
```
Load test case from dataset:
  Question: "What is the maximum engine displacement?"
  Ground Truth: "The maximum engine displacement is 5.0L"

Compare:
  RAG Answer: "The max is 5.0L"
  Ground Truth: "The maximum engine displacement is 5.0L"

Calculate Score:
  Exact Match: 80%
  Semantic Similarity: 100%
  Overall: 90%
  
Result: PASS âœ“
```

**Step 4: Return to Streamlit**
```
{
  "answer": "The max is 5.0L",
  "evaluation": {
    "passed": true,
    "score": 0.90,
    "metrics": {
      "exact_match": 0.8,
      "semantic_similarity": 1.0
    }
  }
}
```

**Step 5: Display in UI**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Answer: The max is 5.0L     â”‚
â”‚ âœ“ Correct (Score: 90%)      â”‚
â”‚                             â”‚
â”‚ Exact Match: 80%            â”‚
â”‚ Semantic Sim: 100%          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¾ What Exactly Gets Stored

### Per Test Case
```
id                    â†’ Unique identifier
question              â†’ The question
ground_truth_answer   â†’ Expected answer
context              â†’ Reference information
difficulty_level     â†’ easy/medium/hard
category             â†’ Topic/subject
expected_sources     â†’ Source documents
metadata             â†’ Custom data (date, author, etc.)
```

### Per Dataset
```
id                   â†’ Unique dataset ID
name                 â†’ Dataset name
description          â†’ What it's for
version              â†’ Version number
status               â†’ active/archived/deprecated
domain               â†’ Domain/category
tags                 â†’ Classification tags
test_case_count      â†’ How many test cases
created_at           â†’ Creation date
updated_at           â†’ Last update date
```

---

## ğŸ—‚ï¸ Storage Structure

```
data/datasets/                          â† All datasets here
â”‚
â”œâ”€â”€ dataset_automotive_qa_1704871200123/
â”‚   â”œâ”€â”€ metadata.json                   â† Dataset info
â”‚   â””â”€â”€ testcases.json                  â† All test cases
â”‚
â”œâ”€â”€ dataset_general_qa_1704871500456/
â”‚   â”œâ”€â”€ metadata.json
â”‚   â””â”€â”€ testcases.json
â”‚
â””â”€â”€ dataset_safety_1704872000789/
    â”œâ”€â”€ metadata.json
    â””â”€â”€ testcases.json
```

---

## ğŸ® Dataset's Role in Query Process

| Phase | What Happens | Dataset Role |
|-------|-------------|--------------|
| **Before Query** | User enters question | Waits (ready for evaluation) |
| **RAG Processing** | Engine retrieves docs + LLM generates | Inactive |
| **After Answer** | RAG produces output | **ACTIVE** - Evaluates output |
| **Evaluation** | Scores calculated | **Provides ground truth** |
| **Result** | User sees answer + score | Comparison complete |

**Key Point**: Dataset is **optional**. Query works without it, but with dataset you get evaluation scores showing if answer is correct.

---

## âœ¨ Key Features

âœ… **Easy to Check**: Single CLI command shows everything  
âœ… **Well Organized**: Stored in dedicated `data/datasets/` folder  
âœ… **Versioned**: Each dataset has version number  
âœ… **Categorized**: Questions grouped by difficulty and category  
âœ… **Flexible**: Can add from CLI, CSV, or API  
âœ… **Evaluates**: Automatically scores RAG output  
âœ… **Optional**: Works with or without dataset  

---

## ğŸš€ Common Tasks

### Check if Dataset Exists
```bash
python scripts/opik/dataset_management.py list-datasets
```

### See What's in a Dataset
```bash
python scripts/opik/dataset_management.py get-dataset --dataset-id <id>
```

### Add Test Cases
```bash
python scripts/opik/dataset_management.py add-test-case \
  --dataset-id <id> \
  --question "Q?" \
  --answer "A."
```

### Evaluate Entire Dataset
```bash
python scripts/opik/dataset_management.py evaluate-dataset \
  --dataset-id <id>
```

---

## ğŸ“š Related Documentation

- [DATASET_QUERY_FLOW.md](DATASET_QUERY_FLOW.md) - Detailed flow explanation
- [DATASET_COMMANDS_REFERENCE.md](DATASET_COMMANDS_REFERENCE.md) - All commands
- [DATASET_HOW_TO_CHECK.md](DATASET_HOW_TO_CHECK.md) - Visual guide to checking datasets
- [DATASETS_IMPLEMENTATION.md](DATASETS_IMPLEMENTATION.md) - Implementation details

---

## â“ FAQ

**Q: Is dataset required?**  
A: No, it's optional. Queries work without it.

**Q: Can I fire query without dataset?**  
A: Yes, you'll get answer but no evaluation score.

**Q: What does evaluation score mean?**  
A: How similar RAG output is to expected answer (0-100%).

**Q: Where is data stored?**  
A: `data/datasets/` folder with JSON files.

**Q: How many entries can I store?**  
A: Unlimited. Limited only by disk space.

**Q: Can I export dataset?**  
A: Yes, to JSON or CSV format.

**Q: Can I import dataset?**  
A: Yes, from JSON or CSV files.

**Q: Does evaluation slow down queries?**  
A: Slightly (optional, disabled by default).

---

## ğŸ¯ Bottom Line

**Dataset's Role in RAG Query Process:**

1. **Stores**: Test cases with questions and expected answers
2. **Located**: `data/datasets/` folder
3. **Usage**: Optional evaluation of RAG output
4. **Evaluation**: Scores how accurate RAG answer is
5. **Query Flow**: Query â†’ RAG generates â†’ Dataset evaluates â†’ Return score

That's it! Simple as that! ğŸ‰

---

For more details, see:
- [DATASET_QUERY_FLOW.md](DATASET_QUERY_FLOW.md) for detailed flow
- [DATASET_COMMANDS_REFERENCE.md](DATASET_COMMANDS_REFERENCE.md) for all commands
- [DATASET_HOW_TO_CHECK.md](DATASET_HOW_TO_CHECK.md) for how to view entries
