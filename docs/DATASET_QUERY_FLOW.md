# ðŸ“Š Dataset in RAG System: Complete Flow Explanation

## ðŸŽ¯ Quick Answer

When you fire a **query in Streamlit**, the dataset works as follows:

```
USER TYPES QUERY IN STREAMLIT
         â†“
    Backend receives query
         â†“
    âœ… [OPTIONAL] Evaluate against Dataset
         â”œâ”€ Compare RAG output with ground truth answers
         â”œâ”€ Calculate accuracy/similarity scores
         â””â”€ Show evaluation results
         â†“
    Return answer to Streamlit UI
```

**Dataset's Role**: Provides test cases (question-answer pairs) to **evaluate and validate** the RAG system's performance.

---

## ðŸ” How to Check Dataset & What Gets Stored

### 1ï¸âƒ£ View All Datasets

**Via CLI:**
```bash
python scripts/opik/dataset_management.py list-datasets
```

**Output:**
```
Datasets:
1. ID: dataset_automotive_test_1234567890
   Name: Automotive Test Dataset
   Version: 1.0.0
   Status: active
   Test Cases: 50
   Created: 2025-01-07T10:30:00

2. ID: dataset_general_qa_9876543210
   Name: General Q&A
   Version: 1.0.0
   Status: active
   Test Cases: 100
   ...
```

### 2ï¸âƒ£ View Specific Dataset Details

**Via CLI:**
```bash
python scripts/opik/dataset_management.py get-dataset --dataset-id dataset_automotive_test_1234567890
```

**Output:**
```
=== Dataset: Automotive Test Dataset ===
ID: dataset_automotive_test_1234567890
Description: Dataset for testing automotive QA
Version: 1.0.0
Status: active
Domain: automotive
Tags: test, v1
Test Cases: 50
Created: 2025-01-07T10:30:00
Updated: 2025-01-07T14:00:00

--- Statistics ---
{
  "total_test_cases": 50,
  "by_difficulty": {
    "easy": 15,
    "medium": 25,
    "hard": 10
  },
  "by_category": {
    "engine": 12,
    "safety": 15,
    "performance": 23
  }
}

--- First 5 Test Cases ---

1. Q: What is the maximum engine displacement?
   A: The maximum engine displacement is 5.0L
   Difficulty: easy, Category: engine

2. Q: How does traction control work?
   A: Traction control prevents wheel slippage by...
   Difficulty: medium, Category: safety

3. Q: Explain regenerative braking system
   A: Regenerative braking recovers kinetic energy...
   Difficulty: hard, Category: performance
   ...
```

### 3ï¸âƒ£ Storage Location

**Local Storage:**
```
data/datasets/
â”œâ”€â”€ dataset_automotive_test_1234567890/
â”‚   â”œâ”€â”€ metadata.json              â† Dataset info
â”‚   â””â”€â”€ testcases.json             â† All test cases
â”œâ”€â”€ dataset_general_qa_9876543210/
â”‚   â”œâ”€â”€ metadata.json
â”‚   â””â”€â”€ testcases.json
â””â”€â”€ ...
```

**Example metadata.json:**
```json
{
  "id": "dataset_automotive_test_1234567890",
  "name": "Automotive Test Dataset",
  "description": "Dataset for testing automotive QA",
  "version": "1.0.0",
  "status": "active",
  "domain": "automotive",
  "tags": ["test", "v1"],
  "test_case_count": 50,
  "created_at": "2025-01-07T10:30:00",
  "updated_at": "2025-01-07T14:00:00"
}
```

**Example testcases.json (first 2 entries):**
```json
[
  {
    "id": "tc_001",
    "question": "What is the maximum engine displacement?",
    "ground_truth_answer": "The maximum engine displacement is 5.0L",
    "context": "Engine specifications document",
    "difficulty_level": "easy",
    "category": "engine",
    "expected_sources": ["engine_specs.pdf"],
    "metadata": {
      "date_added": "2025-01-07",
      "author": "admin"
    }
  },
  {
    "id": "tc_002",
    "question": "How does regenerative braking work?",
    "ground_truth_answer": "Regenerative braking recovers kinetic energy...",
    "context": "Braking system documentation",
    "difficulty_level": "hard",
    "category": "performance",
    "expected_sources": ["braking_system.pdf"],
    "metadata": {
      "date_added": "2025-01-07"
    }
  }
]
```

---

## ðŸ”„ Complete Query Flow with Dataset Role

### Scenario: User Asks Question in Streamlit

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STREAMLIT FRONTEND (User Interface)                         â”‚
â”‚                                                              â”‚
â”‚  User Types: "What is maximum engine displacement?"         â”‚
â”‚         â†“                                                    â”‚
â”‚  [Ask] button clicked                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“ HTTP POST /api/search
        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BACKEND - MAIN API (FastAPI)                                â”‚
â”‚                                                              â”‚
â”‚  @app.post("/api/search")                                   â”‚
â”‚  async def search(query):                                   â”‚
â”‚      # Pass query to RAG engine                             â”‚
â”‚      result = rag_engine.rag_query_complete(query)          â”‚
â”‚           â†“                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                         â”‚
         â†“                         â†“
    
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG ENGINE           â”‚    â”‚ [OPTIONAL] DATASET       â”‚
â”‚                      â”‚    â”‚ EVALUATION               â”‚
â”‚ 1. Retrieve relevant â”‚    â”‚                          â”‚
â”‚    documents from    â”‚    â”‚ â€¢ Load test cases        â”‚
â”‚    FAISS index       â”‚    â”‚ â€¢ Compare RAG output     â”‚
â”‚                      â”‚    â”‚   with ground truth      â”‚
â”‚ 2. Send to LLM with  â”‚    â”‚ â€¢ Calculate scores:      â”‚
â”‚    context           â”‚    â”‚   - Exact match          â”‚
â”‚                      â”‚    â”‚   - Semantic similarity  â”‚
â”‚ 3. Get LLM response  â”‚    â”‚   - Token overlap        â”‚
â”‚    (RAG Answer)      â”‚    â”‚ â€¢ Return evaluation      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   metrics                â”‚
           â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                              â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ RESULTS OBJECT                   â”‚
        â”‚ {                                â”‚
        â”‚   "answer": "The max is 5.0L",  â”‚
        â”‚   "sources": ["engine_specs"],  â”‚
        â”‚   "evaluation": {               â”‚
        â”‚     "passed": true,             â”‚
        â”‚     "score": 0.95,              â”‚
        â”‚     "metrics": {...}            â”‚
        â”‚   }                             â”‚
        â”‚ }                                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“ HTTP Response
        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STREAMLIT FRONTEND                                          â”‚
â”‚                                                              â”‚
â”‚ Display Results:                                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Answer: "The maximum engine displacement is 5.0L"      â”‚ â”‚
â”‚ â”‚ Sources: engine_specs.pdf                              â”‚ â”‚
â”‚ â”‚                                                         â”‚ â”‚
â”‚ â”‚ [EVALUATION RESULTS]                                   â”‚ â”‚
â”‚ â”‚ âœ“ Passed (Score: 0.95)                                 â”‚ â”‚
â”‚ â”‚ Exact Match: 100%                                      â”‚ â”‚
â”‚ â”‚ Semantic Similarity: 90%                               â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“‹ What Gets Stored in Dataset

### Data Stored Per Test Case

Each test case contains:

```python
{
    "id": "tc_001",                                    # Unique identifier
    "question": "What is the max displacement?",       # The question
    "ground_truth_answer": "The max is 5.0L",          # Expected answer
    "context": "Engine specs document",                # Reference context
    "difficulty_level": "easy|medium|hard",            # Difficulty rating
    "category": "engine|safety|performance|...",       # Category/topic
    "expected_sources": ["engine_specs.pdf"],          # Source documents
    "metadata": {
        "date_added": "2025-01-07",
        "author": "admin",
        "custom_field": "value"
    }
}
```

### Dataset Metadata

```python
{
    "id": "dataset_automotive_test_1234567890",
    "name": "Automotive Test Dataset",
    "description": "Test cases for automotive QA",
    "version": "1.0.0",
    "status": "active|archived|deprecated",
    "domain": "automotive",                            # Domain/category
    "tags": ["test", "v1", "production"],              # Tags for filtering
    "test_case_count": 50,                             # Total test cases
    "created_at": "2025-01-07T10:30:00",
    "updated_at": "2025-01-07T14:00:00"
}
```

---

## ðŸŽ® Using Dataset with Queries

### Option 1: Without Dataset Evaluation
```bash
# Just get RAG answer
curl -X POST http://localhost:8001/api/search \
  -H "Content-Type: application/json" \
  -d '{"query": "What is max displacement?"}'

Response:
{
  "answer": "The maximum engine displacement is 5.0L",
  "sources": ["engine_specs.pdf"],
  "tokens_used": 125
}
```

### Option 2: With Dataset Evaluation
```bash
# Evaluate against test case
curl -X POST http://localhost:8001/api/search \
  -H "Content-Type: application/json" \
  -d {
    "query": "What is max displacement?",
    "dataset_id": "dataset_automotive_test_1234567890",
    "test_case_id": "tc_001"
  }

Response:
{
  "answer": "The maximum engine displacement is 5.0L",
  "sources": ["engine_specs.pdf"],
  "evaluation": {
    "test_case_id": "tc_001",
    "passed": true,
    "score": 0.95,
    "predicted_answer": "The maximum engine displacement is 5.0L",
    "ground_truth_answer": "The max is 5.0L",
    "metrics": {
      "exact_match": 0.8,
      "semantic_similarity": 1.0,
      "overall_score": 0.9
    }
  }
}
```

---

## ðŸ”„ Dataset's Role in Query Evaluation

### When Evaluation Happens:

```
1. USER FIRES QUERY IN STREAMLIT
   Query: "What is the maximum engine displacement?"
   
2. RAG ENGINE PROCESSES:
   â€¢ Searches FAISS index for relevant documents
   â€¢ Sends context + query to Groq LLM
   â€¢ Receives answer: "The max is 5.0L"
   
3. DATASET EVALUATION (If Enabled):
   â€¢ Load test case from dataset
   â€¢ Ground truth: "The maximum engine displacement is 5.0L"
   â€¢ Compare RAG output vs Ground truth
   
4. METRICS CALCULATED:
   â€¢ Exact Match: 
     "The max is 5.0L" vs "The maximum engine displacement is 5.0L"
     Result: 0.8 (80% match)
   
   â€¢ Semantic Similarity (token overlap):
     RAG tokens: {the, max, is, 5.0l}
     GT tokens: {the, maximum, engine, displacement, is, 5.0l}
     Overlap: 3/7 = 0.43 (43%)
   
   â€¢ Overall Score: Average = (0.8 + 0.43) / 2 = 0.615
   
   â€¢ Pass/Fail: 
     If score >= 0.5: PASS âœ“
     If score < 0.5: FAIL âœ—
   
5. RETURN RESULTS:
   {
     "answer": "The max is 5.0L",
     "evaluation": {
       "passed": true,
       "score": 0.615,
       "metrics": {...}
     }
   }
```

---

## ðŸ› ï¸ How to Create & Add Test Cases

### Method 1: Via CLI

```bash
# Create a dataset
python scripts/opik/dataset_management.py create-dataset \
  --name "Automotive Q&A" \
  --description "Test cases for automotive questions" \
  --domain automotive \
  --tags test v1

# Add test cases one by one
python scripts/opik/dataset_management.py add-test-case \
  --dataset-id dataset_automotive_qa_1234567890 \
  --question "What is the engine displacement?" \
  --answer "The engine displacement is 5.0L" \
  --context "From engine specs" \
  --difficulty medium \
  --category engine
```

### Method 2: Import from CSV

**testcases.csv:**
```csv
question,ground_truth_answer,context,difficulty_level,category
"What is the engine displacement?","5.0L","Engine specifications","medium","engine"
"How does regenerative braking work?","Recovers kinetic energy...","Braking system docs","hard","performance"
"What is the max speed?","200 mph","Performance specs","easy","performance"
```

```bash
python scripts/opik/dataset_management.py add-from-csv \
  --dataset-id dataset_automotive_qa_1234567890 \
  --file testcases.csv
```

### Method 3: Via REST API

```bash
curl -X POST http://localhost:8001/datasets/create \
  -H "Content-Type: application/json" \
  -d {
    "name": "Automotive Q&A",
    "description": "Automotive test cases",
    "version": "1.0.0",
    "domain": "automotive",
    "tags": ["test", "v1"]
  }

# Response:
{
  "status": "success",
  "dataset_id": "dataset_automotive_qa_1234567890"
}

# Then add test cases:
curl -X POST http://localhost:8001/datasets/dataset_automotive_qa_1234567890/test-cases \
  -H "Content-Type: application/json" \
  -d {
    "question": "What is engine displacement?",
    "ground_truth_answer": "5.0L",
    "difficulty_level": "medium",
    "category": "engine"
  }
```

---

## ðŸ“Š Evaluation Process Detailed

### Exact Match Score

```
Predicted: "The max is 5.0L"
Ground Truth: "The maximum engine displacement is 5.0L"

Comparison (case-insensitive, trimmed):
"the max is 5.0l" == "the maximum engine displacement is 5.0l"
Result: 0.0 (No exact match)

But if prediction was: "The maximum engine displacement is 5.0L"
"the maximum engine displacement is 5.0l" == "the maximum engine displacement is 5.0l"
Result: 1.0 (Perfect match!)
```

### Semantic Similarity (Token Overlap)

```
Predicted: "The max is 5.0L"
Tokens: {the, max, is, 5.0l}

Ground Truth: "The maximum engine displacement is 5.0L"
Tokens: {the, maximum, engine, displacement, is, 5.0l}

Intersection (common tokens): {the, is, 5.0l} = 3 tokens
Union (all unique tokens): {the, max, is, 5.0l, maximum, engine, displacement} = 7 tokens

Jaccard Similarity = Intersection / Union = 3 / 7 = 0.43 (43%)
```

### Overall Score

```
Score = (Exact Match + Semantic Similarity) / 2
      = (0.0 + 0.43) / 2
      = 0.215

Pass Threshold = 0.5
Result: FAIL âœ— (0.215 < 0.5)
```

---

## ðŸ“ˆ Batch Evaluation

Evaluate entire dataset against RAG system:

```bash
# Via CLI
python scripts/opik/dataset_management.py evaluate-dataset \
  --dataset-id dataset_automotive_qa_1234567890

# Output:
Evaluating dataset: dataset_automotive_qa_1234567890
Processing 50 test cases...

Progress: 50/50 [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%

EVALUATION RESULTS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Total Test Cases: 50
Passed: 42 (84%)
Failed: 8 (16%)
Average Score: 0.82

By Difficulty:
  Easy (15 cases):     14 passed (93%)
  Medium (25 cases):   22 passed (88%)
  Hard (10 cases):     6 passed (60%)

By Category:
  Engine (12):        11 passed (92%)
  Safety (15):        12 passed (80%)
  Performance (23):   19 passed (83%)

Summary Report: evaluation_result_1234567890.json
```

---

## ðŸŽ¯ Dataset's Role Summary

| When | What Happens | Dataset Role |
|------|-------------|--------------|
| **User enters query** | Frontend sends to backend | Dataset waits (ready) |
| **RAG processes** | Retrieval + LLM generation | Dataset inactive |
| **Answer generated** | RAG produces output | Dataset evaluates |
| **Evaluation phase** | Compare output vs ground truth | Dataset provides test cases |
| **Metrics calculated** | Accuracy/similarity scores | Dataset provides expected answer |
| **Results returned** | User sees answer + evaluation | Dataset comparison complete |

**In Short**: 
- ðŸ”µ **Before Query**: Dataset stores test cases (expected Q&A pairs)
- ðŸ”µ **During Query**: RAG engine generates answers
- ðŸŸ¢ **After Query**: Dataset evaluates how good the answer is

---

## ðŸ“ Files Involved

| File | Purpose |
|------|---------|
| `src/backend/services/dataset_service.py` | Manages datasets (create, store, retrieve) |
| `src/backend/services/dataset_evaluation.py` | Evaluates RAG output vs ground truth |
| `scripts/opik/dataset_management.py` | CLI for dataset operations |
| `data/datasets/` | Local storage for test cases |
| `src/backend/main.py` | REST API endpoints for datasets |

---

## âœ… Quick Checklist

- [ ] Understand dataset stores Q&A pairs (test cases)
- [ ] Know datasets located in `data/datasets/` 
- [ ] Can create dataset via CLI or API
- [ ] Can add test cases from CSV or manually
- [ ] Understand evaluation metrics (exact match, semantic similarity)
- [ ] Know dataset is optional (query works without it)
- [ ] Can check dataset details with: `python scripts/opik/dataset_management.py get-dataset --dataset-id <id>`

---

Now you understand how datasets work! Any specific part you want me to explain further? ðŸŽ¯
